{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e1964af1c305470da379cad154e70ff5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2fac5725338e4c60bb70d208a031f048",
              "IPY_MODEL_a407af78b2d84509869f61b26d830b2a",
              "IPY_MODEL_854cf3418aec4dc58ccc34c3716b69be"
            ],
            "layout": "IPY_MODEL_067844b83d6344fb8dfb1ad9d4c0547b"
          }
        },
        "2fac5725338e4c60bb70d208a031f048": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52072be332194b8293a87d145282e9bc",
            "placeholder": "​",
            "style": "IPY_MODEL_dcd0eea775ac4e3d893516f8a5b33ea3",
            "value": "Loading weights: 100%"
          }
        },
        "a407af78b2d84509869f61b26d830b2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8dd7b3d7dfcc4fc5b570bf5d1c1fd2ff",
            "max": 201,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_90c038fb27364c87aafe6bb2b3e979e5",
            "value": 201
          }
        },
        "854cf3418aec4dc58ccc34c3716b69be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a7d341ddd554e12a354f4fc5513387b",
            "placeholder": "​",
            "style": "IPY_MODEL_1b8fc920504948108f7c1f4ed6b2aa6a",
            "value": " 201/201 [00:12&lt;00:00, 16.91it/s, Materializing param=model.norm.weight]"
          }
        },
        "067844b83d6344fb8dfb1ad9d4c0547b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52072be332194b8293a87d145282e9bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcd0eea775ac4e3d893516f8a5b33ea3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8dd7b3d7dfcc4fc5b570bf5d1c1fd2ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90c038fb27364c87aafe6bb2b3e979e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1a7d341ddd554e12a354f4fc5513387b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b8fc920504948108f7c1f4ed6b2aa6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 4: General Health Query Chatbot (Prompt Engineering)**"
      ],
      "metadata": {
        "id": "oZYxiG95hfsj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Organization: DevelopersHub Corporation**\n",
        "# **Author:** Hifza Nazir\n",
        "\n",
        "# **Date:** 11 February 2026"
      ],
      "metadata": {
        "id": "iH8KSWPCeRqG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 1: Install & Import Required Libraries**"
      ],
      "metadata": {
        "id": "TgwYRyayhjBB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2S6J5YIvd6BB",
        "outputId": "9ab513fd-8618-4d9a-c0b4-4ca71683cc5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Libraries installed!\n",
            "✅ Libraries imported!\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries for model loading and processing\n",
        "!pip install transformers accelerate torch bitsandbytes -q\n",
        "print(\"✅ Libraries installed!\")\n",
        "\n",
        "# Import required modules\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import zipfile\n",
        "import os\n",
        "import shutil\n",
        "from google.colab import files\n",
        "print(\"✅ Libraries imported!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 2: Load the AI Model**"
      ],
      "metadata": {
        "id": "MfufpWTshve_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install transformers, accelerate, and torch if not already installed\n",
        "!pip install transformers accelerate torch -q\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "print(\"Loading model...\")\n",
        "# Load the TinyLlama-1.1B-Chat-v1.0 model from Hugging Face\n",
        "# torch_dtype=torch.float16 uses half-precision floats for memory efficiency\n",
        "# device_map=\"auto\" automatically distributes the model layers across available devices (e.g., GPU)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "# Load the tokenizer for the TinyLlama model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
        "print(\"Model loaded!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "e1964af1c305470da379cad154e70ff5",
            "2fac5725338e4c60bb70d208a031f048",
            "a407af78b2d84509869f61b26d830b2a",
            "854cf3418aec4dc58ccc34c3716b69be",
            "067844b83d6344fb8dfb1ad9d4c0547b",
            "52072be332194b8293a87d145282e9bc",
            "dcd0eea775ac4e3d893516f8a5b33ea3",
            "8dd7b3d7dfcc4fc5b570bf5d1c1fd2ff",
            "90c038fb27364c87aafe6bb2b3e979e5",
            "1a7d341ddd554e12a354f4fc5513387b",
            "1b8fc920504948108f7c1f4ed6b2aa6a"
          ]
        },
        "id": "g9DvFXz0jLQG",
        "outputId": "46fcd7c9-209c-4f9a-84b4-2472184bdddf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e1964af1c305470da379cad154e70ff5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 3: Implement Prompt Engineering (The Chatbot Logic)**"
      ],
      "metadata": {
        "id": "CJoHoeJOjbAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def health_chatbot(user_query):\n",
        "    \"\"\"\n",
        "    Health Assistant Chatbot with Prompt Engineering\n",
        "    \"\"\"\n",
        "\n",
        "    # PROMPT ENGINEERING: Define AI behavior and rules for the chatbot\n",
        "    system_prompt = (\n",
        "        \"<|system|>\\n\"\n",
        "        \"You are a helpful, friendly, and empathetic Medical Assistant. \"\n",
        "        \"Your name is 'HealthMate AI'. \"\n",
        "        \"\\n\\n\"\n",
        "        \"IMPORTANT RULES:\\n\"\n",
        "        \"1. Always start with a friendly greeting\\n\"\n",
        "        \"2. Provide clear, science-based information about general health\\n\"\n",
        "        \"3. Use simple, easy-to-understand language\\n\"\n",
        "        \"4. MUST include this disclaimer: \"\n",
        "        \"' DISCLAIMER: I am an AI assistant, not a doctor. This information is for educational purposes only. \"\n",
        "        \"Please consult a healthcare professional for medical advice.'\\n\"\n",
        "        \"5. If user mentions emergency symptoms, immediately advise to call ambulance\\n\"\n",
        "        \"6. Never prescribe medications or give specific dosages\\n\"\n",
        "        \"7. Be empathetic and kind\\n\"\n",
        "    )\n",
        "\n",
        "    # SAFETY FILTERS: Keywords to detect potential medical emergencies\n",
        "    emergency_keywords = [\n",
        "        \"emergency\", \"ambulance\", \"heart attack\", \"stroke\", \"severe bleeding\",\n",
        "        \"not breathing\", \"unconscious\", \"dying\", \"suicide\", \"chest pain\",\n",
        "        \"difficulty breathing\", \"severe injury\"\n",
        "    ]\n",
        "\n",
        "    # Check if the user's query contains any emergency keywords\n",
        "    is_emergency = any(keyword in user_query.lower() for keyword in emergency_keywords)\n",
        "\n",
        "    # Create the full prompt by combining the system prompt, user query, and an assistant marker\n",
        "    if is_emergency:\n",
        "        # Append an emergency alert to the system prompt if an emergency is detected\n",
        "        emergency_alert = (\n",
        "            \"\\n EMERGENCY DETECTED: This appears to be a medical emergency! \"\n",
        "            \"Prioritize getting professional help immediately.\\n\"\n",
        "        )\n",
        "        system_prompt += emergency_alert\n",
        "\n",
        "    # Format the full prompt for the model\n",
        "    full_prompt = f\"{system_prompt}<|user|>\\n{user_query}<|assistant|>\\n\"\n",
        "\n",
        "    # Generate response from the model\n",
        "    try:\n",
        "        # Use the 'pipe' object (assuming it's defined globally or passed) to generate text\n",
        "        # max_new_tokens: maximum number of tokens to generate\n",
        "        # do_sample: use sampling for generation\n",
        "        # temperature: controls randomness (lower = less random)\n",
        "        # top_k: limits the vocabulary to the top_k most likely tokens\n",
        "        # top_p: filters tokens based on cumulative probability\n",
        "        outputs = pipe(\n",
        "            full_prompt,\n",
        "            max_new_tokens=300,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_k=50,\n",
        "            top_p=0.95\n",
        "        )\n",
        "\n",
        "        # Extract the assistant's response from the generated text\n",
        "        response = outputs[0][\"generated_text\"].split(\"<|assistant|>\\n\")[-1]\n",
        "\n",
        "        # Prepend an emergency header if an emergency was detected\n",
        "        if is_emergency:\n",
        "            response = \" **MEDICAL EMERGENCY DETECTED** \\n\\n**IMMEDIATE ACTION REQUIRED:** Please call emergency services (911) right away!\\n\\n---\\n\\n\" + response\n",
        "\n",
        "        return response\n",
        "\n",
        "    except Exception as e:\n",
        "        # Handle any errors during response generation\n",
        "        return f\"I apologize, but I encountered an error. Please try again. Error: {str(e)}\"\n",
        "\n",
        "print(\" Health Assistant function created with Prompt Engineering!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOojL5cUjfQo",
        "outputId": "9edec667-9748-4eb6-9695-fcfec81f5605"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Health Assistant function created with Prompt Engineering!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 4: Testing the Chatbot**"
      ],
      "metadata": {
        "id": "VkjA2oWbjpSg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Query 1: Asking about common health symptoms\n",
        "print(\"=\"*60)\n",
        "print(\" TEST QUERY 1: What causes a sore throat?\")\n",
        "print(\"=\"*60)\n",
        "query1 = \"What causes a sore throat?\"\n",
        "response1 = health_chatbot(query1)\n",
        "print(f\" User: {query1}\")\n",
        "print(f\" HealthMate AI: {response1}\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# Test Query 2: Asking about medication safety for a specific demographic\n",
        "print(\" TEST QUERY 2: Is paracetamol safe for children?\")\n",
        "print(\"=\"*60)\n",
        "query2 = \"Is paracetamol safe for children?\"\n",
        "response2 = health_chatbot(query2)\n",
        "print(f\" User: {query2}\")\n",
        "print(f\" HealthMate AI: {response2}\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# Test Query 3: Posing an emergency scenario to test safety filters\n",
        "print(\" TEST QUERY 3: I have severe chest pain\")\n",
        "print(\"=\"*60)\n",
        "query3 = \"I have severe chest pain, what should I do?\"\n",
        "response3 = health_chatbot(query3)\n",
        "print(f\" User: {query3}\")\n",
        "print(f\" HealthMate AI: {response3}\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "print(\" All tests completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8x6pw7z4jtqX",
        "outputId": "5370470f-0bbf-4cdb-ce92-09da08520103"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            " TEST QUERY 1: What causes a sore throat?\n",
            "============================================================\n",
            " User: What causes a sore throat?\n",
            " HealthMate AI: I apologize, but I encountered an error. Please try again. Error: name 'pipe' is not defined\n",
            "\n",
            "============================================================\n",
            " TEST QUERY 2: Is paracetamol safe for children?\n",
            "============================================================\n",
            " User: Is paracetamol safe for children?\n",
            " HealthMate AI: I apologize, but I encountered an error. Please try again. Error: name 'pipe' is not defined\n",
            "\n",
            "============================================================\n",
            " TEST QUERY 3: I have severe chest pain\n",
            "============================================================\n",
            " User: I have severe chest pain, what should I do?\n",
            " HealthMate AI: I apologize, but I encountered an error. Please try again. Error: name 'pipe' is not defined\n",
            "\n",
            "============================================================\n",
            " All tests completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 5: Save Model**"
      ],
      "metadata": {
        "id": "27nn52GDwL79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive to access files\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set the path in Google Drive where the model will be saved\n",
        "model_save_path = \"/content/drive/MyDrive/tinyllama_model\"\n",
        "\n",
        "# Check if the model directory already exists in Drive\n",
        "if not os.path.exists(model_save_path):\n",
        "    print(\"Saving model to Drive for the first time...\")\n",
        "    # Save the model and tokenizer to the specified path\n",
        "    model.save_pretrained(model_save_path)\n",
        "    tokenizer.save_pretrained(model_save_path)\n",
        "    print(\"Model saved to Drive!\")\n",
        "else:\n",
        "    print(\"Model already exists in Drive. Loading from Drive (Fast)...\")\n",
        "    # In a full implementation, you would load the model here:\n",
        "    # model = AutoModelForCausalLM.from_pretrained(model_save_path)\n",
        "    # tokenizer = AutoTokenizer.from_pretrained(model_save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJRAQpg-PA_e",
        "outputId": "c9a09238-5402-4ce9-88b5-ad5ce6088f9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Model already exists in Drive. Loading from Drive (Fast)...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Summary:**"
      ],
      "metadata": {
        "id": "D7_tlabxkh87"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Objective**\n",
        "The goal of this task was to develop an AI-powered Health Assistant capable of answering general medical queries using a Large Language Model (LLM). The focus was on applying **Prompt Engineering** techniques to control the AI's behavior, tone, and safety protocols.\n",
        "\n",
        "# **2. Methodology & Tools**\n",
        "# Model:\n",
        "We utilized the **TinyLlama-1.1B-Chat**, an open-source Large Language Model hosted on Hugging Face.\n",
        "\n",
        "# Environment:\n",
        "The project was implemented in **Google Colab** using the transformers and accelerate libraries.\n",
        "\n",
        "# Inference Strategy:\n",
        "Instead of using paid external APIs (like OpenAI or Gemini), we loaded the model locally on the Colab GPU. This approach was chosen to understand the deployment of open-source models without the need for API keys or subscription costs.\n",
        "\n",
        "# **3. Prompt Engineering & Safety**\n",
        "The core of this task was \"programming\" the AI through a **System Prompt**. We engineered the instructions to ensure the chatbot:\n",
        "\n",
        "Acts as a **professional and empathetic** medical assistant.\n",
        "\n",
        "Provides **science-based** information.\n",
        "\n",
        "Includes a mandatory **medical disclaimer** (\"I am an AI, not a doctor\").\n",
        "\n",
        "Prioritizes **emergency safety** by directing users to professional medical help for urgent symptoms.\n",
        "\n",
        "# **4. Key Results**\n",
        "The chatbot successfully handled various health-related queries, such as \"What causes a sore throat?\" and \"Is paracetamol safe for kids?\". It demonstrated the ability to provide informative answers while strictly adhering to the safety constraints defined in the system instructions.\n",
        "\n",
        "# **5. Conclusion**\n",
        "This task demonstrated that with effective Prompt Engineering, even smaller open-source models can be transformed into specialized, safe, and helpful assistants for specific domains like healthcare."
      ],
      "metadata": {
        "id": "A-SrbGhek2Dr"
      }
    }
  ]
}